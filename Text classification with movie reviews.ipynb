{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification With Movie Reviews\n",
    "\n",
    "#### Given a dataset of movie reviews the objective is to train and test the dataset so the model can predict wheter the review is good (0) or bad (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb # Importing the dataset from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>num_words = 10000</code> variable takes the reviews only 10000 words and refuses the other that have different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the variable train_data is located an integer number that \n",
    "# represents a word, however it needs to be represented in actual text \n",
    "(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words = 88000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the integer numbers into actual words annotations\n",
    "\n",
    "The first is to create a dictionary, fortunately it is already done by Tensorflow.\n",
    "\n",
    "<code>data.get_word_index()</code> function returns tuples of type string so that it can be put on a dictionary to eventually map them and put to each integer value ***v*** a key *(word)* ***k***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index()\n",
    "word_index = {k:(v + 3) for k, v in word_index.items()} # The reason for starting +3 values\n",
    "                                                        # after is because exist some special characters\n",
    "\n",
    "word_index[\"<PAD>\"] = 0  # Padding\n",
    "word_index[\"<START>\"] = 1 # Start\n",
    "word_index[\"<UNK>\"] = 2 # Unknown\n",
    "word_index[\"<UNUSED>\"] =  3 # Unused\n",
    "\n",
    "# Swap all the values in the keys, now the key (word) is first and then the value  \n",
    "# this is basically so the the values of the word (the integers) point to a word (key)\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data annotations\n",
    "\n",
    "Given that we don't know the size and shape of the inputs it is neccesary to know them to start modeling the NN.\n",
    "\n",
    "So that, the padding tag <code> \"PAD\" </code> is used to define a definitive length of all the data, this is, ***it is going to be a allowed a ceratain ammount of words in each review***. In this case the number will be 250 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming the data\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    '''\n",
    "    Function that decodes the text of the movie reviews and returns a\n",
    "    readable words.\n",
    "    \n",
    "    Inputs: The integer values corresponding to a word\n",
    "    \n",
    "    Outputs: The key (word) that corresponds to the integer values    \n",
    "    '''\n",
    "    # If there is a word that is not in the dictionary then it puts\n",
    "    # a question mark (?)\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture annotations\n",
    "\n",
    "The purpose of the model is to find out whether the review is ***GOOD*** or ***BAD*** \n",
    "\n",
    "There are two ways of doing the model for a NN, the first is directly as in [Crash Course TensorFlow](http://localhost:8888/notebooks/Crash%20Course%20Tensorflow%20.ipynb#reference1) and the other is by using the method <code>.add()</code> in which as parameter will get the desired configuration that we want to give to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() # Creating the model \n",
    "model.add(keras.layers.Embedding(88000, 16)) # Hidden layer\n",
    "model.add(keras.layers.GlobalAveragePooling1D()) # Hidden layer\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\")) # Hidden layer\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\")) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers explanation\n",
    "\n",
    "It first starts by taking the input which are the value words and pass it to the *Embedding Layer*.\n",
    "\n",
    "**Embedding Layer:** Takes the input data and turns into many vectors.\n",
    "It also groups the *similar* words \n",
    "\n",
    "**Global Average Pooling:** Takes the vectors and averages them out, kind of shrinks them.\n",
    "\n",
    "**Dense_1:** 16 neurons *(arbitrary networks)* in this case receive the averaged results \n",
    "\n",
    "**Dense_2:** Finally pass through a single neuron that decides whether it is ***GOOD*** (0) or ***BAD*** (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          1408000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,408,289\n",
      "Trainable params: 1,408,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # Useful to visualize how the NN is composed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.6919 - accuracy: 0.5801 - val_loss: 0.6898 - val_accuracy: 0.6709\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 63us/sample - loss: 0.6859 - accuracy: 0.7397 - val_loss: 0.6815 - val_accuracy: 0.6918\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 2s 106us/sample - loss: 0.6735 - accuracy: 0.7533 - val_loss: 0.6667 - val_accuracy: 0.7197\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 2s 157us/sample - loss: 0.6524 - accuracy: 0.7665 - val_loss: 0.6433 - val_accuracy: 0.7665\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 2s 147us/sample - loss: 0.6209 - accuracy: 0.8027 - val_loss: 0.6116 - val_accuracy: 0.7848\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5810 - accuracy: 0.8233 - val_loss: 0.5744 - val_accuracy: 0.7982\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.5362 - accuracy: 0.8391 - val_loss: 0.5351 - val_accuracy: 0.8133\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.4895 - accuracy: 0.8580 - val_loss: 0.4966 - val_accuracy: 0.8280\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.4445 - accuracy: 0.8730 - val_loss: 0.4611 - val_accuracy: 0.8400\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.4033 - accuracy: 0.8832 - val_loss: 0.4309 - val_accuracy: 0.8474\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.3661 - accuracy: 0.8956 - val_loss: 0.4032 - val_accuracy: 0.8581\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.3328 - accuracy: 0.9049 - val_loss: 0.3802 - val_accuracy: 0.8638\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 62us/sample - loss: 0.3041 - accuracy: 0.9123 - val_loss: 0.3616 - val_accuracy: 0.8673\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.2793 - accuracy: 0.9179 - val_loss: 0.3457 - val_accuracy: 0.8727\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2567 - accuracy: 0.9249 - val_loss: 0.3326 - val_accuracy: 0.8748\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.2371 - accuracy: 0.9319 - val_loss: 0.3221 - val_accuracy: 0.8772\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2191 - accuracy: 0.9379 - val_loss: 0.3126 - val_accuracy: 0.8812\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.2033 - accuracy: 0.9423 - val_loss: 0.3048 - val_accuracy: 0.8815\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1886 - accuracy: 0.9480 - val_loss: 0.2986 - val_accuracy: 0.8822\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.1758 - accuracy: 0.9511 - val_loss: 0.2960 - val_accuracy: 0.8828\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.1637 - accuracy: 0.9560 - val_loss: 0.2887 - val_accuracy: 0.8849\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.1526 - accuracy: 0.9607 - val_loss: 0.2865 - val_accuracy: 0.8865\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1432 - accuracy: 0.9635 - val_loss: 0.2822 - val_accuracy: 0.8859\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1337 - accuracy: 0.9673 - val_loss: 0.2797 - val_accuracy: 0.8870\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1247 - accuracy: 0.9701 - val_loss: 0.2781 - val_accuracy: 0.8871\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1165 - accuracy: 0.9741 - val_loss: 0.2764 - val_accuracy: 0.8871\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.1096 - accuracy: 0.9763 - val_loss: 0.2775 - val_accuracy: 0.8876\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1026 - accuracy: 0.9785 - val_loss: 0.2754 - val_accuracy: 0.8891\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.0961 - accuracy: 0.9804 - val_loss: 0.2785 - val_accuracy: 0.8872\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.0904 - accuracy: 0.9827 - val_loss: 0.2751 - val_accuracy: 0.8883\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.0851 - accuracy: 0.9840 - val_loss: 0.2757 - val_accuracy: 0.8886\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.0801 - accuracy: 0.9857 - val_loss: 0.2762 - val_accuracy: 0.8886\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 76us/sample - loss: 0.0753 - accuracy: 0.9865 - val_loss: 0.2768 - val_accuracy: 0.8887\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 77us/sample - loss: 0.0710 - accuracy: 0.9871 - val_loss: 0.2788 - val_accuracy: 0.8892\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 76us/sample - loss: 0.0668 - accuracy: 0.9886 - val_loss: 0.2808 - val_accuracy: 0.8887\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 65us/sample - loss: 0.0631 - accuracy: 0.9893 - val_loss: 0.2820 - val_accuracy: 0.8879\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 52us/sample - loss: 0.0595 - accuracy: 0.9899 - val_loss: 0.2832 - val_accuracy: 0.8884\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.0557 - accuracy: 0.9911 - val_loss: 0.2840 - val_accuracy: 0.8886\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.0528 - accuracy: 0.9915 - val_loss: 0.2882 - val_accuracy: 0.8879\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.0490 - accuracy: 0.9928 - val_loss: 0.2889 - val_accuracy: 0.8879\n",
      "25000/25000 [==============================] - 2s 87us/sample - loss: 2.3398 - accuracy: 0.4970\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "x_val = train_data[:10000] # Taking as validation data\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000] # Taking as validation data\n",
    "y_train = train_labels[10000:]\n",
    "\n",
    "fitModel = model.fit(x_train, y_train, epochs = 40, batch_size = 512, validation_data = (x_val, y_val), verbose = 1)\n",
    "    \n",
    "results = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      " <START> this is a very light headed comedy about a wonderful family that has a son called pecker because he use to peck at his food pecker loves to take all kinds of pictures of the people in a small suburb of baltimore md and manages to get the attention of a group of photo art lovers from new york city pecker has a cute sister who goes simply nuts over sugar and is actually an addict taking spoonfuls of sugar from a bag there are scenes of men showing off the lumps in their jockey's with grinding movements and gals doing pretty much the same it is rather hard to keep your mind out of the gutter with this film but who cares it is only a film to give you a few laughs at a simple picture made in 1998 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction:[0.02796968]\n",
      "Bad Review: 1\n",
      "[2.3398491996002195, 0.497]\n"
     ]
    }
   ],
   "source": [
    "number_of_review = 25\n",
    "test_review = test_data[number_of_review]\n",
    "predict = model.predict([test_review])\n",
    "print(\"Review: \\n\", decode_review(test_review))\n",
    "print(\"Prediction:\" + str(predict[number_of_review]))\n",
    "if test_labels[number_of_review] == 0:\n",
    "    print(\"Good Review: \" + str(test_labels[number_of_review]))\n",
    "else: \n",
    "    print(\"Bad Review: \" + str(test_labels[number_of_review]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading Models \n",
    "### Saving a model\n",
    "\n",
    "The function to save a model is the model's name and the method is:\n",
    "\n",
    "<code>\n",
    "    model.save(\"name.h5\")\n",
    "</code>\n",
    "\n",
    "In order to save models it is necessary to save it as a <code>* .h5</code> file, which will save in binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model\n",
    "\n",
    "In order to save a model the function: \n",
    "\n",
    "<code>\n",
    "    model = keras.models.load_model(\"model.h5\")\n",
    "</code>\n",
    "\n",
    "is useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(string):\n",
    "    encoded = [1]\n",
    "    \n",
    "    for word in string: \n",
    "        if word in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with a new review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seriously, I know that sounds stupid as this is an animated movie, but why can't we have a favorite movie that's animated? If you ask me this is Disney's best animated film of all time. Why do you ask? The animation is just beautiful, the story is powerful and moving, the characters are terrific, the villain is one of Disney's most monstrous and the songs are out of this world incredible! Not only is it my favorite animated movie, this is one of my favorite soundtracks, with the strong power of Elton John The Lion King is absolutely beautiful and a pleasure to watch still to this day. I have to say also that this story is just so beautiful, it's still one of the movies that will always bring a tear to my eye. When I was 17 years old, I begged my dad to take me to the re-release of The Lion King, him and I being the only adults besides the parents who took their children to see the movie, Simba's father dies and I started balling! My dad looks around and said \"Krissy, stop it!\" and I just said \"But it's so saaaaad! Simba's daddy!\", yes I'm a sap, I can't help it, this is a powerful movie.\n",
      "[[    1     2     2   124    15   934   379    17    14     9    35  1125\n",
      "     20    21   138   191    75    28     6   514    20   198     2     2\n",
      "     25   942    72    14     9     2   118  1125    22     7    32    58\n",
      "      2    81    25     2     2   748     9    43   307     4    65     9\n",
      "    976     5   728     4   105    26  1307     4  1019     9    31     7\n",
      "      2    91  9487     5     4   690    26    46     7    14   182  1048\n",
      "      2    64     9    12    61   514  1125    20    14     9    31     7\n",
      "     61   514 16711    19     4   565   671     7     2     2     2     2\n",
      "      2     9   427   307     5     6  1742     8   106   131     8    14\n",
      "    251     2    28     8   135    82    15    14    65     9    43    38\n",
      "    307    45   131    31     7     4   102    15    80   210   721     6\n",
      "   3325     8    61   744     2     2    16  2766   153   154     2 11595\n",
      "     61  1246     8   193    72     8     4     2     7     2     2     2\n",
      "     90     5     2   112     4    64  1473  1371     4   846    37   562\n",
      "     68   476     8    67     4    20     2   336  1442     5     2   645\n",
      "  33735     2  1246   272   187     5   301     2   570     2     5     2\n",
      "     43   301     2    45    38     2     2     2   422     2     6 10531\n",
      "      2   191   339    12    14     9     6   976    20     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "[0.99407506]\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "          for line in f.readlines():\n",
    "              nline = line.replace(\",\",\"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\":\",\"\").replace(\"/\",\"\").replace(\"!\",\"\").strip().split(\" \")\n",
    "              encode = review_encode(nline)\n",
    "              encode = keras.preprocessing.sequence.pad_sequences([encode], value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)\n",
    "              predict = model.predict(encode)\n",
    "              print(line)\n",
    "              print(encode)\n",
    "              print(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.compile of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f4b5637a490>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
